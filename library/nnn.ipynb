{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJTDiVH-i-NR"
   },
   "source": [
    "# EDSA 2022 Classification Hackathon - South African Language Identification\n",
    "\n",
    "**Overview**\n",
    "\n",
    "South Africa is a multicultural society that is characterised by its rich linguistic diversity. Language is an indispensable tool that can be used to deepen democracy and also contribute to the social, cultural, intellectual, economic and political life of the South African society.\n",
    "\n",
    "The country is multilingual with 11 official languages, each of which is guaranteed equal status. Most South Africans are multilingual and able to speak at least two or more of the official languages.\n",
    "From South African Government\n",
    "\n",
    "With such a multilingual population, it is only obvious that our systems and devices also communicate in multi-languages.\n",
    "\n",
    "In this challenge, you will take text which is in any of South Africa's 11 Official languages and identify which language the text is in. This is an example of NLP's Language Identification, the task of determining the natural language that a piece of text is written in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Najw70_j5Ez"
   },
   "source": [
    "# Honour Code\n",
    "\n",
    "I **Christian Divinefavour**, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the EDSA honour code.\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "\n",
    "# Problem Statement\n",
    "\n",
    "With the divers official languages in South Africa, 11, precisely, a system is needed to effectively taken in texts in any of this languages and identify accurateky which language it's in; this is to aid general communal interaction in the country.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mK8VCw5Go5F3"
   },
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Preprocessing</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCWduVQUuWSK"
   },
   "source": [
    "<a id=\"one\"></a>\n",
    "## 1. Importing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mA2giLmbj95r"
   },
   "outputs": [],
   "source": [
    "\n",
    "#import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DELuv83DwC3b"
   },
   "source": [
    "<a id = \"two\"></a>\n",
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRs0a1EFzPwN"
   },
   "source": [
    "**Load** **original** **data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "RhvXSe13vo-_",
    "outputId": "628060a1-cfe5-4f11-8804-9e197655780d"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_set.csv')\n",
    "df_test = pd.read_csv('test_set.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hn2myFEhwt1I",
    "outputId": "64ac2b9d-9719-44ed-a0d3-1c0c34319ad9"
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EX-maJYky4ih"
   },
   "source": [
    "**Create a copy of data for further analysis and grouping by distinct language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7SA3sDwczc44"
   },
   "outputs": [],
   "source": [
    "train_copy = df_train.copy()\n",
    "test_copy = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOOEcgxvx1rN"
   },
   "source": [
    "<a id = \"three\"></a>\n",
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIAUs5Te0ZE1"
   },
   "source": [
    "**Now that we've successfully loaded our data, let's see what we're working with; let's check for data types on train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UTlAzkS0YdK",
    "outputId": "729ade02-1c51-4471-df06-dc559b9810bf"
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoQAFuA1z9ZS"
   },
   "source": [
    "**Next, let's check for null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wnxSbWdxqMl",
    "outputId": "100ee47d-afc7-459a-9538-b7e23c52c279"
   },
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U43yJ0C31JIH"
   },
   "source": [
    "**There are no null values in the data frame. Let's check for unique values of lang_id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_UDq4FX0MRL",
    "outputId": "d53fbaef-1d5d-4694-9c6a-714390a2b798"
   },
   "outputs": [],
   "source": [
    "unique_vals = df_train['lang_id'].unique()\n",
    "count_of_unique_vals = df_train['lang_id'].nunique()\n",
    "\n",
    "print(unique_vals, \"\\nThere are \", count_of_unique_vals, \"unique values in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVQVrrSF_Uci"
   },
   "source": [
    "**Let's visualize the how these languages occur in the data frame by plot the values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "Uv4c3xWw1-cP",
    "outputId": "3153e63a-eb05-45de-82c2-5c037df9a95f"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([1,1,1,1])\n",
    "df_train['lang_id'].value_counts().plot(kind = 'barh')\n",
    "ax.set_xlabel('Languages')\n",
    "ax.set_ylabel('Count of Languages')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2cyh450EInO"
   },
   "source": [
    "**It seems each language has 3000 entries. Let's confirming that by creating a dictionary to show each language and the number of occurences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rdq-PA1l9XPi",
    "outputId": "5eff21b5-0848-4cbe-804e-534a4270ffdd"
   },
   "outputs": [],
   "source": [
    "unique_plot = {}\n",
    "for i in unique_vals:\n",
    "    unique_plot[i] = df_train[df_train['lang_id'] == i]['lang_id'].count()\n",
    "unique_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5_S5hmzEgzL"
   },
   "source": [
    "**Clearly they have 3000 entries, each**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAFiN5irFpZm"
   },
   "source": [
    "**Let us now visualize what the words look like, using word clouds.**\n",
    "\n",
    "1. Group the data into a list of 11 dataframes of the unique languages\n",
    "2. Visualize, using word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tw4m4wqzEBdE",
    "outputId": "0c337cf2-9e63-4ca8-9a12-6c45e31dd77d"
   },
   "outputs": [],
   "source": [
    "column_list = [train_copy[train_copy['lang_id'] == j] for j in unique_vals]\n",
    "column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-qaCeu96Ha2_",
    "outputId": "b9ddb980-fc00-4daa-84de-1d7f961a5f05"
   },
   "outputs": [],
   "source": [
    "#xho' 'eng' 'nso' 'ven' 'tsn' 'nbl' 'zul' 'ssw' 'tso' 'sot' 'afr'\n",
    "#--VISUALIZATION\n",
    "xho = train_copy[(train_copy['lang_id'] == 'xho')]\n",
    "eng = train_copy[(train_copy['lang_id'] == 'eng')]\n",
    "nso = train_copy[(train_copy['lang_id'] == 'nso')]\n",
    "ven = train_copy[(train_copy['lang_id'] == 'ven')]\n",
    "tsn = train_copy[(train_copy['lang_id'] == 'tsn')]\n",
    "nbl = train_copy[(train_copy['lang_id'] == 'nbl')]\n",
    "zul = train_copy[(train_copy['lang_id'] == 'zul')]\n",
    "ssw = train_copy[(train_copy['lang_id'] == 'ssw')]\n",
    "tso = train_copy[(train_copy['lang_id'] == 'tso')]\n",
    "sot = train_copy[(train_copy['lang_id'] == 'sot')]\n",
    "afr = train_copy[(train_copy['lang_id'] == 'afr')]\n",
    "\n",
    "plt.figure()\n",
    "wc = WordCloud(max_words = 200).generate(\" \".join(xho.text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.title('XHO',fontsize=20)\n",
    "\n",
    "plt.figure()\n",
    "wc = WordCloud(max_words = 200).generate(\" \".join(eng.text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.title('ENG',fontsize=20)\n",
    "\n",
    "plt.figure()\n",
    "wc = WordCloud(max_words = 200).generate(\" \".join(nso.text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.title('NSO',fontsize=20)\n",
    "\n",
    "plt.figure()\n",
    "wc = WordCloud(max_words = 200).generate(\" \".join(ven.text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.title('VEN',fontsize=20)\n",
    "\n",
    "plt.figure()\n",
    "wc = WordCloud(max_words = 200).generate(\" \".join(tsn.text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.title('TSN',fontsize=20)\n",
    "\n",
    "plt.figure()\n",
    "wc = WordCloud(max_words = 200).generate(\" \".join(nbl.text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.title('NBL',fontsize=20)\n",
    "\n",
    "plt.figure()\n",
    "wc = WordCloud(max_words = 200).generate(\" \".join(zul.text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.title('ZUL',fontsize=20)\n",
    "\n",
    "plt.figure()\n",
    "wc = WordCloud(max_words = 200).generate(\" \".join(ssw.text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.title('SSW',fontsize=20)\n",
    "\n",
    "plt.figure()\n",
    "wc = WordCloud(max_words = 200).generate(\" \".join(tso.text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.title('TSO',fontsize=20)\n",
    "\n",
    "plt.figure()\n",
    "wc = WordCloud(max_words = 200).generate(\" \".join(sot.text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.title('SOT',fontsize=20)\n",
    "\n",
    "plt.figure()\n",
    "wc = WordCloud(max_words = 200).generate(\" \".join(afr.text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.title('AFR',fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s88sYgetqfQq"
   },
   "source": [
    "# 4. Data Processing\n",
    "\n",
    "Let's get to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qmEy9_bOKV-Z",
    "outputId": "cc4ce220-a83e-41bb-ac7a-559d324487f8"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1bp-nkBq4T4",
    "outputId": "132a07a1-f0aa-44f1-9ae9-2f5d5d96357b"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from bs4 import BeautifulSoup\n",
    "def review_to_words(raw_message):\n",
    "    # 1. Delete HTML \n",
    "    message_text = BeautifulSoup(raw_message, 'html.parser').get_text()\n",
    "    #letters2 =raw_message.replace('http\\S+|www.\\S+', '', case=False)\n",
    "    # 2. Make a space\n",
    "    letters3 = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', message_text)\n",
    "    letters_only = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))', '', letters3, flags=re.MULTILINE)\n",
    "    letters = re.sub('[^a-zA-Z]', ' ',  letters_only)\n",
    "    letters1 = re.sub(r'http', ' ', letters)\n",
    "    \n",
    "    letters2 = re.sub(\"\\n\", \"\", letters1)\n",
    "     \n",
    "    # 3. lower letters\n",
    "    words = letters2.lower().split()\n",
    "    # 5. Stopwords \n",
    "    meaningful_words = [w for w in words if not w in stop]\n",
    "    # 6. lemmitization\n",
    "    lemmitize_words = [lemmatizer.lemmatize(w) for w in meaningful_words]\n",
    "    # 7. space join words\n",
    "    return( ' '.join(lemmitize_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXL5kRXbsMaB"
   },
   "outputs": [],
   "source": [
    "df_train['cleaned_text'] = df_train['text'].apply(review_to_words)\n",
    "df_test['cleaned_text'] = df_train['text'].apply(review_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "C42rWJQisui9",
    "outputId": "443b2dbc-21e5-41af-843c-658b8dca6b21"
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "63eTfm-xtY_w",
    "outputId": "d6ccd80c-1851-4cd5-c2e8-fb56a018b34a"
   },
   "outputs": [],
   "source": [
    "use_train = df_train[['lang_id', 'cleaned_text']]\n",
    "use_test = df_test[['cleaned_text']]\n",
    "\n",
    "use_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbwDG-Uts6Tp",
    "outputId": "b44d68da-917d-4a65-fb7f-218a9d2bb43e"
   },
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer(max_features=20000,analyzer='word', ngram_range=(2, 2))\n",
    "tfidf1 = TfidfVectorizer()\n",
    "vect1 = [count_vector , tfidf1]\n",
    "\n",
    "X = count_vector.fit_transform(use_train['cleaned_text'].values.astype(str))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq1wYyMRyKv5"
   },
   "outputs": [],
   "source": [
    "y = use_train['lang_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "IVm-dzPPyrKe",
    "outputId": "c69382a4-ec59-4cc4-b3a4-10c2d0e32020"
   },
   "outputs": [],
   "source": [
    "train_model = pd.DataFrame(data=X.toarray(),columns = count_vector.get_feature_names())\n",
    "train_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKrij0Ley24o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IAmmdGWzxfl"
   },
   "source": [
    "## 5. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnqFRIBez04R"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_model, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Epa8gvwz-r_"
   },
   "outputs": [],
   "source": [
    "rF_model = RandomForestClassifier(n_estimators=2, random_state=0)\n",
    "rF_model.fit(train_model, y)\n",
    "y_pred = rF_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spuD0rzE0o2z"
   },
   "outputs": [],
   "source": [
    "classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emnXwied2BXD"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNhnGMTaEH0v"
   },
   "outputs": [],
   "source": [
    "svc =  LinearSVC(C= 1)\n",
    "p = [0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "svc.fit(train_model, y)\n",
    "y_pred1 = rF_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "logreg  = MultinomialNB()\n",
    "logreg.fit(train_model, y)\n",
    "y_pred2 = rF_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhJOGa6t__51"
   },
   "source": [
    "Let's repeat all we've done on train_df to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7aqMFafBVS4"
   },
   "outputs": [],
   "source": [
    "use_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzgbU05jA8FZ"
   },
   "outputs": [],
   "source": [
    "count_test = count_vector.transform(use_test['cleaned_text'].values.astype(str))\n",
    "count_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJ97V03uBrMy"
   },
   "outputs": [],
   "source": [
    "test_model = pd.DataFrame(data = count_test.toarray(),columns = count_vector.get_feature_names())\n",
    "test_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1truvZjB4tW"
   },
   "outputs": [],
   "source": [
    "rF_pred = rF_model.predict(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fidIx1FBCAAs"
   },
   "outputs": [],
   "source": [
    "n =  test_model.index.tolist()\n",
    "index = [x+1 for x in n]\n",
    "\n",
    "submission = pd.DataFrame({'index': index, 'lang_id': rF_pred})\n",
    "submission.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDQWQZMACweB"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ufix_NkHC52w"
   },
   "outputs": [],
   "source": [
    "svc_pred = svc.predict(test_model)\n",
    "submission1 = pd.DataFrame({'index': index, 'lang_id': svc_pred})\n",
    "submission1.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLbDEFlHE199"
   },
   "outputs": [],
   "source": [
    "log_pred = logreg.predict(test_model)\n",
    "submission3 = pd.DataFrame({'index': index, 'lang_id': log_pred})\n",
    "submission3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
